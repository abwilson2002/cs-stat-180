{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fafe9e99",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhodes-byu/cs-stat-180/blob/main/notebooks/05-data-preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfea1ba",
      "metadata": {
        "id": "edfea1ba"
      },
      "source": [
        "\n",
        "# From Mess to Model: Mastering Data Preparation\n",
        "\n",
        "This notebook accompanies the lecture and provides runnable examples covering:\n",
        "- Data discovery & profiling  \n",
        "- Cleaning (missing values, inconsistencies, duplicates, outliers)  \n",
        "- Transformation (scaling, aggregation)  \n",
        "- Feature engineering  \n",
        "- A mini case study for customer churn preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b29e1e8b",
      "metadata": {
        "id": "b29e1e8b"
      },
      "source": [
        "## Setup: Loading Files into Colab environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uSkC1B2sqif2",
      "metadata": {
        "id": "uSkC1B2sqif2"
      },
      "source": [
        "### **Method 1**: The Easiest Way (Using Google Colab)\n",
        "If you are working in a Google Colab notebook, this is the simplest and most secure method. Colab is designed to integrate seamlessly with your Google Drive.\n",
        "\n",
        "* **Step 1**: Mount Your Google Drive\n",
        "Run the following code in a Colab cell. It will prompt you to authorize access to your Google Drive. You'll click a link, sign in to your Google account, and copy-paste the authorization code back into the cell.\n",
        "\n",
        "* After this runs, you'll see a new drive folder in the file explorer on the left, which contains all of your Google Drive files.\n",
        "\n",
        "* **Step 2**: Find the File Path\n",
        "Navigate through the file explorer to find your desired file. Right-click on the file and select \"Copy path\".\n",
        "\n",
        "* **Step 3**: Read the File with Pandas\n",
        "Now, use the copied path inside the pd.read_csv() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8qUdoZu0pA1L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qUdoZu0pA1L",
        "outputId": "288bb8c5-4557-4bd6-e89f-7cdd8aa90b82"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZgfMJq0npCk_",
      "metadata": {
        "id": "ZgfMJq0npCk_"
      },
      "outputs": [],
      "source": [
        "#filepath to customers.csv file. /content/drive/MyDrive/BYU/CS 180/Data/customers.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LlYCqmCjqIFf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlYCqmCjqIFf",
        "outputId": "87ca22b5-ce2f-4156-cd14-443aba126b18"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/cfb16.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Paste the path you copied here\u001b[39;00m\n\u001b[32m      4\u001b[39m file_path = \u001b[33m'\u001b[39m\u001b[33m/content/drive/MyDrive/BYU/CS 180/Data/customers.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/cfb16.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Display the first 5 rows to confirm it loaded correctly\u001b[39;00m\n\u001b[32m      9\u001b[39m df.head()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abwil\\miniconda3\\envs\\JupyterProject\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abwil\\miniconda3\\envs\\JupyterProject\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abwil\\miniconda3\\envs\\JupyterProject\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abwil\\miniconda3\\envs\\JupyterProject\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abwil\\miniconda3\\envs\\JupyterProject\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/cfb16.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paste the path you copied here\n",
        "file_path = '/content/drive/MyDrive/BYU/CS 180/Data/customers.csv'\n",
        "\n",
        "df = pd.read_csv(data/cfb16.csv)\n",
        "\n",
        "# Display the first 5 rows to confirm it loaded correctly\n",
        "df.head()\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NTTWjYfQrbQi",
      "metadata": {
        "id": "NTTWjYfQrbQi"
      },
      "source": [
        "### **Method 2**: The Shareable Link Method (Works Anywhere)\n",
        "This method works in any Python environment (like a local Jupyter Notebook or a script) and doesn't require installing special libraries, but it does require you to change the sharing settings of your file.\n",
        "\n",
        "* **Step 1**: Share the File in Google Drive\n",
        "Go to your Google Drive, right-click on worldcities.csv, and select Share. In the sharing settings, change the access from \"Restricted\" to \"Anyone with the link\".\n",
        "\n",
        "* **Step 2**: Copy the Link and Extract the File ID\n",
        "Copy the shareable link. It will look like this:\n",
        "https://drive.google.com/file/d/SOME_LONG_FILE_ID/view?usp=sharing\n",
        "\n",
        "The important part is the FILE_ID, which is the long string of characters between /d/ and /view.\n",
        "\n",
        "* **Step 3**: Construct the Direct Download URL and Read with Pandas\n",
        "You can use the FILE_ID to create a direct download link. The following Python code does this for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d30aaa",
      "metadata": {
        "id": "f4d30aaa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 120)\n",
        "\n",
        "# Paste the FILE_ID you extracted from the shareable link\n",
        "file_id = '1-qH2pQIP9LNe3XO6mxLH9wlbbfTlT-8W'\n",
        "\n",
        "# This creates the direct download URL\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first 5 rows to confirm it loaded correctly\n",
        "\n",
        "print(\"Shape:\",df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74614111",
      "metadata": {
        "id": "74614111"
      },
      "source": [
        "\n",
        "## Introduction: Why Data Preparation?\n",
        "\n",
        "> *Garbage in, garbage out.* Models cannot fix bad data.  \n",
        "Data preparation typically consumes a large proportion of project time and directly impacts model accuracy and reliability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb8c5b4",
      "metadata": {
        "id": "5fb8c5b4"
      },
      "source": [
        "## Step 1: Data Profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4ace4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "id": "5a4ace4c",
        "outputId": "a0208c36-95a3-4a7c-88b5-8f203f16d1e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Inspect structure, types, and missingness\n",
        "df.info()\n",
        "display(df.isna().sum())\n",
        "df.describe()\n",
        "\n",
        "# Random sample 5 of rows\n",
        "df.sample(5, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11aa0616",
      "metadata": {
        "id": "11aa0616"
      },
      "source": [
        "## Step 2: Data Cleaning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j26npEkbwrfW",
      "metadata": {
        "id": "j26npEkbwrfW"
      },
      "source": [
        "### Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d915820",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d915820",
        "outputId": "2ce8ef38-6179-4d82-c72a-e00ae2404be2"
      },
      "outputs": [],
      "source": [
        "# Example strategies:\n",
        "# 1) Deletion (listwise) — only when safe and small proportion missing\n",
        "df_del = df.dropna(subset=['Age', 'Income'], how='any')\n",
        "print(\"Listwise deletion shape:\", df_del.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U747aND6S5w5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U747aND6S5w5",
        "outputId": "fa719ea2-4ee4-4749-933f-b543b5cfbcca"
      },
      "outputs": [],
      "source": [
        "# 2) Deletion (column-wise) - when a large proportion is missing\n",
        "# Calculate the percentage of missing values for each column\n",
        "missing_percentage = df.isnull().sum() / len(df) * 100\n",
        "\n",
        "# Identify columns with missing percentage greater than 10%\n",
        "cols_to_drop = missing_percentage[missing_percentage > 10].index\n",
        "\n",
        "# Drop these columns from the DataFrame\n",
        "df_col_dropped = df.drop(columns=cols_to_drop)\n",
        "\n",
        "print(\"Original shape:\", df.shape)\n",
        "print(\"Shape after dropping columns with >10% missing values:\", df_col_dropped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C3NaAoo-To6C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3NaAoo-To6C",
        "outputId": "a2cb45cb-a98e-4d02-9ed5-b317bf218723"
      },
      "outputs": [],
      "source": [
        "# 3) Deletion (listwise) - only when safe and small proportion missing\n",
        "df_del_any = df.dropna(how='any')\n",
        "print(\"Listwise deletion (any NaN) shape:\", df_del_any.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-BEFUXlwvbCJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "-BEFUXlwvbCJ",
        "outputId": "c7b56f7a-6b4a-449d-a75d-61774b1efb8f"
      },
      "outputs": [],
      "source": [
        "# 4) Simple imputation\n",
        "df_imp = df.copy()\n",
        "df_imp['Age'] = df_imp['Age'].fillna(df_imp['Age'].median())\n",
        "df_imp['Income'] = df_imp['Income'].fillna(df_imp['Income'].median())\n",
        "df_imp[['Age','Income']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TVs9VAX4veeb",
      "metadata": {
        "id": "TVs9VAX4veeb"
      },
      "outputs": [],
      "source": [
        "# 5) Predictive imputation can be done with models (skipped for brevity here)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfeb7690",
      "metadata": {
        "id": "cfeb7690"
      },
      "source": [
        "### Inaccurate & Inconsistent Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uTE1fbiny4DZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTE1fbiny4DZ",
        "outputId": "425fb41a-5827-4993-c760-24272572462d"
      },
      "outputs": [],
      "source": [
        "df_imp['State'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85339a26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85339a26",
        "outputId": "72b5cfc6-1b9a-4bcb-eb96-62687509c38d"
      },
      "outputs": [],
      "source": [
        "# Standardize State column (CA, California, Calif. -> CA)\n",
        "state_map = {'California': 'CA', 'Calif.': 'CA'}\n",
        "df_imp['State'] = df_imp['State'].replace(state_map)\n",
        "df_imp['State'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rM10ZWFTx95V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "rM10ZWFTx95V",
        "outputId": "d02e7822-e1fe-4fc7-90b2-983baf020971"
      },
      "outputs": [],
      "source": [
        "display(df_imp[['JoinDate','LastPurchaseDate']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y_ysyMfKwpg2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "y_ysyMfKwpg2",
        "outputId": "048f09b9-22c2-4300-f4c2-3affa28dfc16"
      },
      "outputs": [],
      "source": [
        "# Ensure dates are parsed and consistent\n",
        "\n",
        "df_imp['JoinDate'] = pd.to_datetime(df_imp['JoinDate'], errors='coerce')\n",
        "df_imp['LastPurchaseDate'] = pd.to_datetime(df_imp['LastPurchaseDate'], errors='coerce')\n",
        "df_imp.info()\n",
        "df_imp[['JoinDate','LastPurchaseDate']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g3K3duBvxYUo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "g3K3duBvxYUo",
        "outputId": "8a7b26df-0ddc-4f82-9343-0583b4ecd58d"
      },
      "outputs": [],
      "source": [
        "df_imp['Name'].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hgj0KY8GwsNj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "hgj0KY8GwsNj",
        "outputId": "3fe6670e-d119-40bb-e269-d79d9e6f315d"
      },
      "outputs": [],
      "source": [
        "# Trim/canonicalize text fields (example: Name)\n",
        "df_imp['Name'] = df_imp['Name'].str.strip()\n",
        "\n",
        "df_imp[['State','JoinDate','LastPurchaseDate','Name']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "005df2c0",
      "metadata": {
        "id": "005df2c0"
      },
      "source": [
        "### Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b320d810",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b320d810",
        "outputId": "12adf9c6-ff09-4494-89e7-fbc220476dbb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Count duplicates (all columns)\n",
        "dup_count = df_imp.duplicated().sum()\n",
        "print(\"Exact duplicate rows:\", dup_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FQWvEM8ixobZ",
      "metadata": {
        "id": "FQWvEM8ixobZ"
      },
      "outputs": [],
      "source": [
        "# Identify potential dupes by a key subset (e.g., Name + JoinDate + State)\n",
        "subset_dupes = df_imp.duplicated(subset=['Name','JoinDate','State']).sum()\n",
        "print(\"Subset-based duplicate rows:\", subset_dupes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5F_HyY1Rxqpo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F_HyY1Rxqpo",
        "outputId": "e3f1175d-25a8-4256-d1b3-ef5846cd08c5"
      },
      "outputs": [],
      "source": [
        "# Drop exact duplicates\n",
        "df_nodup = df_imp.drop_duplicates()\n",
        "df_nodup.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e04899b7",
      "metadata": {
        "id": "e04899b7"
      },
      "source": [
        "### Outliers (Z-score & IQR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef1cbbd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef1cbbd0",
        "outputId": "33ad85cc-dee9-4e88-c294-931faf2e62fc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Z-score method for TotalSpend\n",
        "ts = df_nodup['TotalSpend']\n",
        "z = (ts - ts.mean()) / ts.std(ddof=0)\n",
        "outlier_idx = np.where(np.abs(z) > 3)[0]\n",
        "print(\"Z-score outlier indices (|z| > 3):\", outlier_idx.tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zpu58Qlux2Ur",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "Zpu58Qlux2Ur",
        "outputId": "ca8b0209-f533-4cf1-f879-fd8a15d98a08"
      },
      "outputs": [],
      "source": [
        "# IQR method\n",
        "Q1, Q3 = ts.quantile(0.25), ts.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
        "iqr_outliers = df_nodup[(ts < lower) | (ts > upper)]\n",
        "print(\"IQR outliers:\")\n",
        "display(iqr_outliers[['CustomerID','Name','TotalSpend']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eriEw3bx4Rw",
      "metadata": {
        "id": "9eriEw3bx4Rw"
      },
      "outputs": [],
      "source": [
        "# Option: cap outliers at bounds (winsorization-style)\n",
        "df_capped = df_nodup.copy()\n",
        "df_capped['TotalSpend_capped'] = df_capped['TotalSpend'].clip(lower=lower, upper=upper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qJ0r9KSpx6kN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "qJ0r9KSpx6kN",
        "outputId": "325add96-e19f-4415-9c5b-774a163679bd"
      },
      "outputs": [],
      "source": [
        "# Quick boxplot before/after (for class visualization)\n",
        "plt.figure()\n",
        "plt.boxplot([df_nodup['TotalSpend'].values, df_capped['TotalSpend_capped'].values], labels=['Raw','Capped'])\n",
        "plt.title('TotalSpend: Raw vs Capped')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "759a1397",
      "metadata": {
        "id": "759a1397"
      },
      "source": [
        "## Step 3: Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a3c7e0",
      "metadata": {
        "id": "43a3c7e0"
      },
      "source": [
        "### Normalization (Min-Max) vs Standardization (Z-score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9563cbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "e9563cbc",
        "outputId": "6bc1a4c1-d44c-48d0-f534-4a8287702b8a"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "scale_df = df_capped[['Income','TotalSpend_capped']].dropna().copy()\n",
        "\n",
        "# Min-Max Scaling\n",
        "mm = MinMaxScaler()\n",
        "scale_df['Income_MinMax'] = mm.fit_transform(scale_df[['Income']])\n",
        "scale_df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wtf0VsXbyE68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Wtf0VsXbyE68",
        "outputId": "c634116d-6945-4eaf-c0b0-d277d0f0fffc"
      },
      "outputs": [],
      "source": [
        "# Standardization\n",
        "ss = StandardScaler()\n",
        "scale_df['Income_Standard'] = ss.fit_transform(scale_df[['Income']])\n",
        "scale_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014b0e15",
      "metadata": {
        "id": "014b0e15"
      },
      "source": [
        "### Data Aggregation (monthly sales example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8183693c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "8183693c",
        "outputId": "4fb2a151-3866-4d96-e896-6b8097f77711"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Aggregate by month using LastPurchaseDate and TotalSpend\n",
        "agg = df_capped.dropna(subset=['LastPurchaseDate']).copy()\n",
        "agg['Month'] = agg['LastPurchaseDate'].dt.to_period('M')\n",
        "monthly_sales = agg.groupby('Month', as_index=False)['TotalSpend'].sum()\n",
        "\n",
        "print(\"Monthly sales (sum of TotalSpend by last purchase month):\")\n",
        "display(monthly_sales)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y230HdEkySwE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Y230HdEkySwE",
        "outputId": "9ca514d7-12d5-4d48-f237-599ddc5351f6"
      },
      "outputs": [],
      "source": [
        "# Simple plot\n",
        "plt.figure()\n",
        "plt.plot(monthly_sales['Month'].astype(str), monthly_sales['TotalSpend'])\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Monthly Sales')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('TotalSpend')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10211b9f",
      "metadata": {
        "id": "10211b9f"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a227f51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "8a227f51",
        "outputId": "8df020e7-7231-4d6a-f3f6-efa063bf6e46"
      },
      "outputs": [],
      "source": [
        "fe = df_capped.copy()\n",
        "\n",
        "# Day of week from LastPurchaseDate\n",
        "fe['DayOfWeek'] = fe['LastPurchaseDate'].dt.day_name()\n",
        "\n",
        "# Days since last purchase (relative to \"today\")\n",
        "today = pd.Timestamp.today().normalize()\n",
        "fe['DaysSinceLastPurchase'] = (today - fe['LastPurchaseDate']).dt.days\n",
        "\n",
        "# Simple CLV proxy: average monthly spend over recency (protect against div by zero)\n",
        "fe['MonthsSinceLastPurchase'] = np.maximum(fe['DaysSinceLastPurchase'] / 30.0, 1.0)\n",
        "fe['CLV_proxy'] = fe['TotalSpend'] / fe['MonthsSinceLastPurchase']\n",
        "\n",
        "fe[['CustomerID','Name','DayOfWeek','DaysSinceLastPurchase','CLV_proxy']].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f209557",
      "metadata": {
        "id": "2f209557"
      },
      "source": [
        "## Mini Case Study: Preparing Customer Data for Churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ae78d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "21ae78d9",
        "outputId": "d1dff4b4-4f64-4873-d2d5-0119e69d97ed"
      },
      "outputs": [],
      "source": [
        "# Start from raw\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "raw = pd.read_csv(url)\n",
        "\n",
        "# Profile the data\n",
        "raw.head()\n",
        "raw.info()\n",
        "raw.describe()\n",
        "raw.isnull().sum()\n",
        "\n",
        "# 1) Make categorical names consistent and convert dates (stings) to datetime\n",
        "raw['State'] = raw['State'].replace({'California':'CA','Calif.':'CA'})\n",
        "raw['JoinDate'] = pd.to_datetime(raw['JoinDate'], errors='coerce')\n",
        "raw['LastPurchaseDate'] = pd.to_datetime(raw['LastPurchaseDate'], errors='coerce')\n",
        "\n",
        "# 2) Impute numeric columns (median)\n",
        "for col in ['Age','Income']:\n",
        "    raw[col] = raw[col].fillna(raw[col].median())\n",
        "\n",
        "# 3) Handle missing LastPurchaseDate: fill with JoinDate as fallback (domain choice for demo)\n",
        "raw['LastPurchaseDate'] = raw['LastPurchaseDate'].fillna(raw['JoinDate'])\n",
        "\n",
        "# 4) Remove duplicates\n",
        "clean = raw.drop_duplicates()\n",
        "\n",
        "# 5) Feature engineering\n",
        "today = pd.Timestamp.today().normalize()\n",
        "clean['DaysSinceLastPurchase'] = (today - clean['LastPurchaseDate']).dt.days\n",
        "clean['TenureDays'] = (today - clean['JoinDate']).dt.days\n",
        "clean['AvgMonthlySpend'] = clean['TotalSpend'] / np.maximum(clean['TenureDays']/30.0, 1.0)\n",
        "\n",
        "# Simple CLV proxy: AvgMonthlySpend * 6 months horizon\n",
        "clean['CustomerLifetimeValue_6mo'] = clean['AvgMonthlySpend'] * 6\n",
        "\n",
        "# 6) Scaling selected features (for model input)\n",
        "for_scale = clean[['Income','TotalSpend','DaysSinceLastPurchase','TenureDays','CustomerLifetimeValue_6mo']].copy()\n",
        "scaler = StandardScaler()\n",
        "scaled = pd.DataFrame(scaler.fit_transform(for_scale), columns=[c + \"_z\" for c in for_scale.columns])\n",
        "\n",
        "model_input = pd.concat([clean[['CustomerID','Churn','State']], scaled], axis=1)\n",
        "model_input.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf12cd2",
      "metadata": {
        "id": "8bf12cd2"
      },
      "source": [
        "\n",
        "## Conclusion & Key Takeaways\n",
        "\n",
        "- Data preparation is iterative: profile → clean → transform → engineer → (repeat as needed).  \n",
        "- Document your assumptions and choices (e.g., why you imputed a value a certain way).  \n",
        "- High-quality preparation underpins trustworthy, impactful models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uSkC1B2sqif2",
        "NTTWjYfQrbQi"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "JupyterProject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
